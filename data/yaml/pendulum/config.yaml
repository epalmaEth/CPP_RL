runner:
  # -- Env
  num_envs: 1024
  seed: 0
  # -- Learning
  max_iterations: 1000  # number of policy updates
  num_steps_per_env: 50
  # -- Saving
  save_interval: 10000
  save_dir: "data/policies/"
  # -- Loading
  load_dir: "data/policies/"
  # -- Logging
  logging_buffer: 1
ppo:
  # -- Value loss 
  value_loss_coef: 1.0
  clip_param: 0.2
  use_clipped_value_loss: true
  # -- Surrogate loss
  desired_kl: 0.01
  entropy_coef: 0.01
  gamma: 0.99
  lam: 0.95
  max_grad_norm: 1.0
  # -- Training
  learning_rate: 0.001
  num_epochs: 5
  num_batches: 4
  learning_rate_schedule: "adaptive" # "adaptive" or "fixed"
actor:
  normalizer:
    type: "identity" # "identity" or "empirical"
  mlp:
    width: 1 # i-th next power of 2 
    depth: 3
    activation: "relu" # "relu" or "tanh"
  distribution:
    init_noise_std: 1.0
    type: "normal" # "normal" or "beta"
critic:
  normalizer:
    type: "identity" # "identity" or "empirical"
  mlp:
    width: 1 # i-th next power of 2 
    depth: 3
    activation: "relu" # "relu" or "tanh"